# Build manylinux wheels, and upload them to the build for testing within a short timeframe
name: Manylinux2014

# Run on branch push events (i.e. not tag pushes) and on pull requests
on:
  push:
    branches:
      - '**'
    paths:
      - "**"
      - "!.github/**"
      - ".github/workflows/Manylinux2014.yml"
  pull_request:

defaults:
  run:
    shell: bash

# A single job, which builds manylinux2014 wheels, which ships with GCC 10.2.1 at the time of writing. If this bumps to unpatched 10.3 we might have issues w/ cuda. 
jobs:
  build:
    runs-on: ${{ matrix.cudacxx.os }}
    # Run steps inside a manylinux container.
    container: quay.io/pypa/manylinux2014_x86_64
    strategy:
      fail-fast: false
      # Multiplicative build matrix
      # optional exclude: can be partial, include: must be specific
      matrix:
        cudacxx:
          - cuda: "11.2"
            cuda_arch: "35"
            hostcxx: devtoolset-9
            os: ubuntu-20.04
          # - cuda: "11.0"
          #   cuda_arch: "35"
          #   hostcxx: devtoolset-9
          #   os: ubuntu-20.04
          # - cuda: "10.0"
          #   cuda_arch: "35"
          #   hostcxx: devtoolset-7
          #   os: ubuntu-18.04
        python: 
          # - "3.9"
          - "3.8"
          # - "3.7"
          # - "3.6"
        config:
          - name: "Release"
            config: "Release"
            SEATBELTS: "ON"
          # - name: "SEATBELTS=OFF"
          #   config: "Release"
          #   SEATBELTS: "OFF"
          # - name: "Debug"
          #   config: "Debug"
          #   SEATBELTS: "OFF"
        VISUALISATION: 
          # - "ON"
          - "OFF"

    # Name the job based on matrix/env options
    name: "build (${{ matrix.cudacxx.cuda }}, ${{matrix.python}}, ${{ matrix.VISUALISATION }}, ${{ matrix.config.name }}, ${{ matrix.cudacxx.os }})"

    env:
      # Control if the wheel should be repaired. This will fail until .so's are addressed
      AUDITWHEEL_REPAIR: "OFF"
      MANYLINUX: "manylinux2014"
      ARCH: "x86_64"
      # Define constants
      BUILD_DIR: "build"
      BUILD_TESTS: "OFF"
      # Conditional based on matrix via awkward almost ternary
      BUILD_SWIG_PYTHON: ${{ fromJSON('{true:"ON",false:"OFF"}')[matrix.python != ''] }}
      # Port matrix options to environment, for more portability.
      CUDA: ${{ matrix.cudacxx.cuda }}
      CUDA_ARCH: ${{ matrix.cudacxx.cuda_arch }}
      HOSTCXX: ${{ matrix.cudacxx.hostcxx }}
      OS: ${{ matrix.cudacxx.os }}
      CONFIG: ${{ matrix.config.config }}
      SEATBELTS: ${{ matrix.config.SEATBELTS }}
      PYTHON: ${{ matrix.python}}
      VISUALISATION: ${{ matrix.VISUALISATION }}

    steps:
    - uses: actions/checkout@v2

    # Downgrade the devtoolset in the image based on the build matrix, using:
    # gcc-10 for CUDA >= 11.2. Unclear if devtoolset-10 will upgrade to unpatched 11.3 which breaks CUDA builds that use <chrono>. 
    # gcc-9 for CUDA >= 11.0
    # gcc-8 for CUDA >= 10.1
    # gcc-7 for CUDA >= 10.0 (and probably 9.x).
    # these are not the officially supported toolset on centos by cuda, but it's what works.
    - name: Install RHEL devtoolset (CentOS)
      if: ${{ startsWith(env.HOSTCXX, 'devtoolset-') }}
      run: |
        # Install devtoolset-X
        yum install -y ${{ env.HOSTCXX }}
        # Enable the toolset via source not scl enable which doesn't get on with multi-step GHA 
        source /opt/rh/${{ env.HOSTCXX }}/enable
        # Export the new environment / compilers for subsequent steps.
        echo "PATH=${PATH}" >> $GITHUB_ENV
        echo "CC=$(which gcc)" >> $GITHUB_ENV
        echo "CXX=$(which g++)" >> $GITHUB_ENV
        echo "CUDAHOSTCXX=$(which g++)" >> $GITHUB_ENV

    - name: Install CUDA (CentOS)
      if: ${{ env.CUDA != '' }}
      env:
        cuda: ${{ env.CUDA }}
      run: .github/scripts/install_cuda_centos.sh

    - name: Configure cmake
      run: >
        cmake . -B "${{ env.BUILD_DIR }}"
        -DCMAKE_BUILD_TYPE=${{ env.CONFIG }}
        -DCMAKE_WARN_DEPRECATED=OFF 
        -DCUDA_ARCH="${{ env.CUDA_ARCH }} "
        -DBUILD_TESTS="${{ env.BUILD_TESTS }} "
        -DBUILD_SWIG_PYTHON="${{ env.BUILD_SWIG_PYTHON }}"
        -DPYTHON3_EXACT_VERSION=${{ env.PYTHON }}
        -DVISUALISATION="${{ env.VISUALISATION }}"

    - name: Build python wheel
      if: ${{ env.BUILD_SWIG_PYTHON == 'ON' }}
      working-directory: ${{ env.BUILD_DIR }}
      run: cmake --build . --target pycdga --verbose -j `nproc`

    # Run audithweel show for information, but do not repair.
    - name: Run auditwheel show
      working-directory: ${{ env.BUILD_DIR }}
      run: auditwheel show lib/${{ env.CONFIG }}/python/dist/*.whl

    # Manually extract the wheel, and use patchelf to remove the dependnce on libcuda.so, before repackaging the wheel. 
    - name: patchelf remove, auditwheel repair, patchelff add
      working-directory: ${{ env.BUILD_DIR }}
      run: |
        # Setup list of .so's to remove then re-add
        so_to_remove=( libcuda.so libcudart.so )
        so_to_readd=()
        ###########
        # Remove cuda .so dependencies
        ###########
        # Grab the first (hopefully only) wheel in the directory.
        wheelpath=$(find lib/Release/python/dist/ -name "*.whl" | head -n 1)
        # Extract the wheel into a temp dir.
        extracted_dir=extracted
        unzip -o ${wheelpath} -d ${extracted_dir}
        # Switch into the extracted directory
        pushd ${extracted_dir}
        # Find the .so 
        sofile=$(find . -name "_*.so" | head -n 1)
        # List needed .so's 
        patchelf --print-needed ${sofile}
        # Remove .so via patchelf, remembering the actual soname
        for sopattern in "${so_to_remove}"; do
            soname=$(ldd ${sofile} | grep ${sopattern} | sed -e 's/^[[:space:]]*//' | cut -d " " -f 1 | head -n 1)
            echo "removing ${soname}"
            patchelf --remove-needed ${soname} ${sofile}
            so_to_readd+=($soname)
        done
        # List updated list of needed .so's 
        patchelf --print-needed ${sofile}
        # Repackage the wheel.
        wheelname=$(basename ${wheelpath})
        zip -r ${wheelname} *
        # Pop out the directory
        popd
        # Move the mutated wheel
        mv ${extracted_dir}/${wheelname} ${wheelpath}
        # Clean up the extracted dir
        rm -r ${extracted_dir}


        ###########
        # Package non-cuda .so dependencies
        ###########
        # Run auditwheel repair
        auditwheel repair --plat ${{ env.MANYLINUX }}_${{ env.ARCH }} lib/${{ env.CONFIG }}/python/dist/*whl --no-update-tags -w lib/${{ env.CONFIG }}/python/dist 
    

        ###########
        # re-add cuda .so dependencies
        ###########
        # Grab the first (hopefully only) wheel in the directory.
        wheelpath=$(find lib/Release/python/dist/ -name "*.whl" | head -n 1)
        # Extract the wheel into a temp dir.
        extracted_dir=extracted
        unzip -o ${wheelpath} -d ${extracted_dir}
        # Switch into the extracted directory
        pushd ${extracted_dir}
        # Find the .so 
        sofile=$(find . -name "_*.so" | head -n 1)
        # List needed .so's 
        patchelf --print-needed ${sofile}
        # Re-add .so via patchelf
        for soname in "${so_to_readd}"; do
            echo "re-adding ${soname}"
            patchelf --add-needed ${soname} ${sofile}
        done
        # List updated list of needed .so's 
        patchelf --print-needed ${sofile}
        # Repackage the wheel.
        wheelname=$(basename ${wheelpath})
        zip -r ${wheelname} *
        # Pop out the directory
        popd
        # Move the mutated wheel
        mv ${extracted_dir}/${wheelname} ${wheelpath}
        # Clean up the extracted dir
        rm -r ${extracted_dir}


    # Ideally we should use auditwheel repair to check/enforce conformity
    # But we cannot due to cuda shared object (libcuda.so.1) dependencies which we cannot/shouldnot/wil not package into the wheel. 
    - name: Run auditwheel repair
      if: ${{ env.AUDITWHEEL_REPAIR == 'ON' }}
      working-directory: ${{ env.BUILD_DIR }}
      run: auditwheel repair --plat ${{ env.MANYLINUX }}_${{ env.ARCH }} lib/${{ env.CONFIG }}/python/dist/*.whl -w lib/${{ env.CONFIG }}/python/dist


    # Upload wheel artifacts to the job on GHA, for a very short duration. This might not be desirable in practice.
    - name: Upload Wheel Artifacts
      uses: actions/upload-artifact@v2
      with:
        name: wheelhouse
        path: ${{ env.BUILD_DIR }}/lib/${{ env.CONFIG }}/python/dist/*.whl
        if-no-files-found: error
        retention-days: 7
